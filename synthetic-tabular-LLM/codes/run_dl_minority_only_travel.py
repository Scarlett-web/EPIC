"""
ğŸ¯ Travel æ•°æ®é›†æ·±åº¦å­¦ä¹ åŸºçº¿ - åªç”Ÿæˆå°‘æ•°ç±»æ ·æœ¬ï¼ˆæ¡ä»¶é‡‡æ ·ï¼‰
ä½¿ç”¨ CTGAN å’Œ TVAE çš„æ¡ä»¶é‡‡æ ·åŠŸèƒ½ï¼Œåªç”Ÿæˆ 'Target=1' ç±»åˆ«çš„æ ·æœ¬
"""
import pandas as pd
import numpy as np
import time
import warnings
import sys

# å¼ºåˆ¶åˆ·æ–°è¾“å‡º
sys.stdout.reconfigure(line_buffering=True)

from sdv.single_table import CTGANSynthesizer, TVAESynthesizer
from sdv.metadata import SingleTableMetadata
from sdv.sampling import Condition
from xgboost import XGBClassifier
from sklearn.metrics import f1_score, balanced_accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder

warnings.filterwarnings('ignore')

# ================= é…ç½®åŒºåŸŸ =================
DATA_DIR = '../data/realdata/travel'
SAVE_DIR = '../data/syndata'
TARGET_COLUMN = 'Target'
MINORITY_CLASS = 1  # å°‘æ•°ç±»æ ‡ç­¾ï¼ˆè´­ä¹°ä¿é™©ï¼‰
# ===========================================

print("=" * 70, flush=True)
print("ğŸ¯ Travel æ•°æ®é›†æ·±åº¦å­¦ä¹ åŸºçº¿ - åªç”Ÿæˆå°‘æ•°ç±»æ ·æœ¬ï¼ˆæ¡ä»¶é‡‡æ ·ï¼‰", flush=True)
print("=" * 70, flush=True)

# 1. è¯»å–æ•°æ®
print("\n[1/7] æ­£åœ¨è¯»å–æ•°æ®...", flush=True)
X_train = pd.read_csv(f'{DATA_DIR}/X_train.csv', index_col=0)
y_train = pd.read_csv(f'{DATA_DIR}/y_train.csv', index_col=0)
X_test = pd.read_csv(f'{DATA_DIR}/X_test.csv', index_col=0)
y_test = pd.read_csv(f'{DATA_DIR}/y_test.csv', index_col=0).values.ravel()

print(f"   è®­ç»ƒé›†: {X_train.shape[0]} æ¡, æµ‹è¯•é›†: {X_test.shape[0]} æ¡", flush=True)

# åˆå¹¶æ•°æ®
train_data = pd.concat([X_train, y_train], axis=1)

# æŸ¥çœ‹åŸå§‹ç±»åˆ«åˆ†å¸ƒ
print(f"\n   ğŸ“Š åŸå§‹è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:", flush=True)
class_counts = y_train[TARGET_COLUMN].value_counts()
for cls, count in class_counts.items():
    print(f"      Target={cls}: {count} æ¡ ({count/len(y_train)*100:.2f}%)", flush=True)

# è®¡ç®—éœ€è¦ç”Ÿæˆçš„å°‘æ•°ç±»æ ·æœ¬æ•°
majority_count = class_counts.max()
minority_count = class_counts.min()
samples_needed = int(majority_count - minority_count)  # è½¬æ¢ä¸º Python int

print(f"\n   ğŸ¯ éœ€è¦ç”Ÿæˆ {samples_needed} æ¡ 'Target={MINORITY_CLASS}' æ ·æœ¬ä»¥å¹³è¡¡æ•°æ®é›†", flush=True)

# æ ‡ç­¾ç¼–ç 
le_y = LabelEncoder()
y_test_enc = le_y.fit_transform(y_test)
minority_class_idx = np.argmin(np.bincount(y_test_enc))

# 2. æ£€æµ‹å…ƒæ•°æ®
print("\n[2/7] æ­£åœ¨æ£€æµ‹æ•°æ®ç»“æ„...", flush=True)
metadata = SingleTableMetadata()
metadata.detect_from_dataframe(train_data)
print("   âœ… å…ƒæ•°æ®æ£€æµ‹å®Œæˆ", flush=True)

# è¯„ä¼°å‡½æ•°
def evaluate_balanced_data(name, synthetic_minority_data):
    """è¯„ä¼°ä½¿ç”¨åˆæˆå°‘æ•°ç±»æ ·æœ¬å¹³è¡¡åçš„æ•°æ®"""
    print(f"\n   ğŸ“Š è¯„ä¼° {name} ç”Ÿæˆçš„å¹³è¡¡æ•°æ®...", flush=True)
    
    # 1. åˆå¹¶åŸå§‹è®­ç»ƒæ•°æ®å’Œåˆæˆå°‘æ•°ç±»æ•°æ®
    X_syn = synthetic_minority_data.drop(TARGET_COLUMN, axis=1)
    y_syn = synthetic_minority_data[TARGET_COLUMN]
    
    print(f"   ç”Ÿæˆçš„å°‘æ•°ç±»æ ·æœ¬: {len(X_syn)} æ¡", flush=True)
    print(f"   å°‘æ•°ç±»åˆ†å¸ƒ: {dict(y_syn.value_counts())}", flush=True)
    
    # 2. åˆ›å»ºå¹³è¡¡çš„è®­ç»ƒé›†
    X_balanced = pd.concat([X_train, X_syn], axis=0)
    y_balanced = pd.concat([y_train, y_syn], axis=0)
    
    print(f"\n   å¹³è¡¡åè®­ç»ƒé›†å¤§å°: {len(X_balanced)} æ¡", flush=True)
    print(f"   å¹³è¡¡åç±»åˆ«åˆ†å¸ƒ:", flush=True)
    balanced_counts = y_balanced[TARGET_COLUMN].value_counts()
    for cls, count in balanced_counts.items():
        print(f"      Target={cls}: {count} æ¡ ({count/len(y_balanced)*100:.2f}%)", flush=True)
    
    # 3. æ•°æ®ç¼–ç 
    full_X = pd.concat([X_balanced, X_test], axis=0)
    encoders = {}
    
    for col in X_train.columns:
        if X_train[col].dtype == 'object' or X_train[col].nunique() < 10:
            le = LabelEncoder()
            full_X[col] = full_X[col].astype(str).fillna('Missing')
            le.fit(full_X[col])
            encoders[col] = le
    
    # ç¼–ç è®­ç»ƒæ•°æ®
    X_balanced_enc = X_balanced.copy()
    for col, le in encoders.items():
        X_balanced_enc[col] = le.transform(X_balanced_enc[col].astype(str).fillna('Missing'))
    
    y_balanced_enc = le_y.transform(y_balanced.values.ravel())
    
    # 4. è®­ç»ƒ XGBoost
    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, verbosity=0)
    model.fit(X_balanced_enc, y_balanced_enc)
    
    # 5. é¢„æµ‹
    X_test_encoded = X_test.copy()
    for col, le in encoders.items():
        X_test_encoded[col] = le.transform(X_test_encoded[col].astype(str).fillna('Missing'))
    
    y_pred = model.predict(X_test_encoded)
    
    # 6. è¯„ä¼°æŒ‡æ ‡
    f1 = f1_score(y_test_enc, y_pred, pos_label=minority_class_idx)
    ba = balanced_accuracy_score(y_test_enc, y_pred)
    
    print(f"\n   âœ… F1 Score: {f1:.4f}", flush=True)
    print(f"   âœ… Balanced Accuracy: {ba:.4f}", flush=True)
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print(f"\n   ğŸ“‹ åˆ†ç±»æŠ¥å‘Š:", flush=True)
    report = classification_report(y_test_enc, y_pred, target_names=[str(c) for c in le_y.classes_], digits=4)
    print(report, flush=True)
    
    return f1, ba

# ===========================================
# ğŸ¤– æ–¹æ³• 1: CTGAN æ¡ä»¶é‡‡æ ·
# ===========================================
print("\n" + "=" * 70, flush=True)
print("ğŸ¤– [3/7] å¼€å§‹è®­ç»ƒ CTGANï¼ˆæ¡ä»¶é‡‡æ ·æ¨¡å¼ï¼‰", flush=True)
print("=" * 70, flush=True)
print(f"   è®­ç»ƒè½®æ•°: 100", flush=True)
print(f"   ç›®æ ‡: åªç”Ÿæˆ {samples_needed} æ¡ 'Target={MINORITY_CLASS}' æ ·æœ¬", flush=True)

start_time = time.time()
ctgan = CTGANSynthesizer(metadata, epochs=100, verbose=True)
print("   å¼€å§‹è®­ç»ƒ...", flush=True)
ctgan.fit(train_data)
train_time = time.time() - start_time

print(f"\n   âœ… CTGAN è®­ç»ƒå®Œæˆï¼è€—æ—¶: {train_time/60:.2f} åˆ†é’Ÿ", flush=True)
print(f"   æ­£åœ¨ä½¿ç”¨æ¡ä»¶é‡‡æ ·ç”Ÿæˆ 'Target={MINORITY_CLASS}' ç±»æ ·æœ¬...", flush=True)

# ğŸ”¥ å…³é”®ï¼šä½¿ç”¨æ¡ä»¶é‡‡æ ·åªç”Ÿæˆå°‘æ•°ç±»
condition = Condition(num_rows=samples_needed, column_values={TARGET_COLUMN: MINORITY_CLASS})
syn_ctgan_minority = ctgan.sample_from_conditions(conditions=[condition])

print(f"   âœ… ç”Ÿæˆå®Œæˆï¼", flush=True)
print(f"   éªŒè¯ç”Ÿæˆçš„æ ·æœ¬ç±»åˆ«:", flush=True)
print(f"   {dict(syn_ctgan_minority[TARGET_COLUMN].value_counts())}", flush=True)

# ä¿å­˜
syn_ctgan_minority.to_csv(f"{SAVE_DIR}/Travel_CTGAN_minority_only.csv", index=False)
print(f"   âœ… å·²ä¿å­˜: {SAVE_DIR}/Travel_CTGAN_minority_only.csv", flush=True)

f1_ctgan, ba_ctgan = evaluate_balanced_data("CTGAN (æ¡ä»¶é‡‡æ ·)", syn_ctgan_minority)

# ===========================================
# ğŸ¤– æ–¹æ³• 2: TVAE æ¡ä»¶é‡‡æ ·
# ===========================================
print("\n" + "=" * 70, flush=True)
print("ğŸ¤– [4/7] å¼€å§‹è®­ç»ƒ TVAEï¼ˆæ¡ä»¶é‡‡æ ·æ¨¡å¼ï¼‰", flush=True)
print("=" * 70, flush=True)

start_time = time.time()
tvae = TVAESynthesizer(metadata, epochs=100, verbose=True)
print("   å¼€å§‹è®­ç»ƒ...", flush=True)
tvae.fit(train_data)
train_time = time.time() - start_time

print(f"\n   âœ… TVAE è®­ç»ƒå®Œæˆï¼è€—æ—¶: {train_time/60:.2f} åˆ†é’Ÿ", flush=True)
print(f"   æ­£åœ¨ä½¿ç”¨æ¡ä»¶é‡‡æ ·ç”Ÿæˆ 'Target={MINORITY_CLASS}' ç±»æ ·æœ¬...", flush=True)

# ğŸ”¥ å…³é”®ï¼šä½¿ç”¨æ¡ä»¶é‡‡æ ·åªç”Ÿæˆå°‘æ•°ç±»
syn_tvae_minority = tvae.sample_from_conditions(conditions=[condition])

print(f"   âœ… ç”Ÿæˆå®Œæˆï¼", flush=True)
print(f"   éªŒè¯ç”Ÿæˆçš„æ ·æœ¬ç±»åˆ«:", flush=True)
print(f"   {dict(syn_tvae_minority[TARGET_COLUMN].value_counts())}", flush=True)

# ä¿å­˜
syn_tvae_minority.to_csv(f"{SAVE_DIR}/Travel_TVAE_minority_only.csv", index=False)
print(f"   âœ… å·²ä¿å­˜: {SAVE_DIR}/Travel_TVAE_minority_only.csv", flush=True)

f1_tvae, ba_tvae = evaluate_balanced_data("TVAE (æ¡ä»¶é‡‡æ ·)", syn_tvae_minority)

# ===========================================
# ğŸ¤– æ–¹æ³• 3: CTGAN æ‹’ç»é‡‡æ ·ï¼ˆRejection Samplingï¼‰
# ===========================================
print("\n" + "=" * 70, flush=True)
print("ğŸ¤– [5/7] CTGAN æ‹’ç»é‡‡æ ·æ–¹æ³•ï¼ˆç”Ÿæˆåç­›é€‰ï¼‰", flush=True)
print("=" * 70, flush=True)
print(f"   ç­–ç•¥: ç”Ÿæˆå¤§é‡æ ·æœ¬ï¼Œç„¶ååªä¿ç•™ 'Target={MINORITY_CLASS}' ç±»", flush=True)

# ç”Ÿæˆæ›´å¤šæ ·æœ¬ä»¥ç¡®ä¿æœ‰è¶³å¤Ÿçš„å°‘æ•°ç±»
oversample_factor = 5  # ç”Ÿæˆ 5 å€çš„æ ·æœ¬
total_samples = samples_needed * oversample_factor

print(f"   ç”Ÿæˆ {total_samples} æ¡æ ·æœ¬ï¼Œç„¶åç­›é€‰å‡º {samples_needed} æ¡ 'Target={MINORITY_CLASS}' æ ·æœ¬...", flush=True)

syn_ctgan_all = ctgan.sample(num_rows=total_samples)
syn_ctgan_rejected = syn_ctgan_all[syn_ctgan_all[TARGET_COLUMN] == MINORITY_CLASS].head(samples_needed)

print(f"\n   âœ… æ‹’ç»é‡‡æ ·å®Œæˆï¼", flush=True)
print(f"   åŸå§‹ç”Ÿæˆ: {len(syn_ctgan_all)} æ¡", flush=True)
print(f"   å…¶ä¸­ 'Target={MINORITY_CLASS}': {(syn_ctgan_all[TARGET_COLUMN] == MINORITY_CLASS).sum()} æ¡", flush=True)
print(f"   ä¿ç•™: {len(syn_ctgan_rejected)} æ¡", flush=True)

# ä¿å­˜
syn_ctgan_rejected.to_csv(f"{SAVE_DIR}/Travel_CTGAN_rejection_sampling.csv", index=False)
print(f"   âœ… å·²ä¿å­˜: {SAVE_DIR}/Travel_CTGAN_rejection_sampling.csv", flush=True)

f1_ctgan_rej, ba_ctgan_rej = evaluate_balanced_data("CTGAN (æ‹’ç»é‡‡æ ·)", syn_ctgan_rejected)

# ===========================================
# ğŸ¤– æ–¹æ³• 4: TVAE æ‹’ç»é‡‡æ ·ï¼ˆRejection Samplingï¼‰
# ===========================================
print("\n" + "=" * 70, flush=True)
print("ğŸ¤– [6/7] TVAE æ‹’ç»é‡‡æ ·æ–¹æ³•ï¼ˆç”Ÿæˆåç­›é€‰ï¼‰", flush=True)
print("=" * 70, flush=True)

print(f"   ç”Ÿæˆ {total_samples} æ¡æ ·æœ¬ï¼Œç„¶åç­›é€‰å‡º {samples_needed} æ¡ 'Target={MINORITY_CLASS}' æ ·æœ¬...", flush=True)

syn_tvae_all = tvae.sample(num_rows=total_samples)
syn_tvae_rejected = syn_tvae_all[syn_tvae_all[TARGET_COLUMN] == MINORITY_CLASS].head(samples_needed)

print(f"\n   âœ… æ‹’ç»é‡‡æ ·å®Œæˆï¼", flush=True)
print(f"   åŸå§‹ç”Ÿæˆ: {len(syn_tvae_all)} æ¡", flush=True)
print(f"   å…¶ä¸­ 'Target={MINORITY_CLASS}': {(syn_tvae_all[TARGET_COLUMN] == MINORITY_CLASS).sum()} æ¡", flush=True)
print(f"   ä¿ç•™: {len(syn_tvae_rejected)} æ¡", flush=True)

# å¦‚æœç”Ÿæˆçš„å°‘æ•°ç±»æ ·æœ¬ä¸å¤Ÿï¼Œç»§ç»­ç”Ÿæˆ
if len(syn_tvae_rejected) < samples_needed:
    print(f"   âš ï¸ è­¦å‘Šï¼šç”Ÿæˆçš„ 'Target={MINORITY_CLASS}' æ ·æœ¬ä¸è¶³ï¼Œéœ€è¦ç»§ç»­ç”Ÿæˆ...", flush=True)
    additional_needed = samples_needed - len(syn_tvae_rejected)
    additional_samples = tvae.sample(num_rows=additional_needed * 20)  # ç”Ÿæˆæ›´å¤š
    additional_minority = additional_samples[additional_samples[TARGET_COLUMN] == MINORITY_CLASS].head(additional_needed)
    syn_tvae_rejected = pd.concat([syn_tvae_rejected, additional_minority], axis=0)
    print(f"   âœ… è¡¥å……å®Œæˆï¼Œæœ€ç»ˆä¿ç•™: {len(syn_tvae_rejected)} æ¡", flush=True)

# ä¿å­˜
syn_tvae_rejected.to_csv(f"{SAVE_DIR}/Travel_TVAE_rejection_sampling.csv", index=False)
print(f"   âœ… å·²ä¿å­˜: {SAVE_DIR}/Travel_TVAE_rejection_sampling.csv", flush=True)

f1_tvae_rej, ba_tvae_rej = evaluate_balanced_data("TVAE (æ‹’ç»é‡‡æ ·)", syn_tvae_rejected)

# ===========================================
# ğŸ“Š æœ€ç»ˆæ€»ç»“
# ===========================================
print("\n" + "=" * 70, flush=True)
print("âœ… [7/7] æ‰€æœ‰æ–¹æ³•è¿è¡Œå®Œæ¯•ï¼", flush=True)
print("=" * 70, flush=True)

print(f"\nğŸ“Š æœ€ç»ˆç»“æœå¯¹æ¯”:", flush=True)
print(f"\n{'æ–¹æ³•':<30} {'F1 Score':<12} {'Balanced Acc':<15}", flush=True)
print("-" * 70, flush=True)
print(f"{'CTGAN (æ¡ä»¶é‡‡æ ·)':<30} {f1_ctgan:<12.4f} {ba_ctgan:<15.4f}", flush=True)
print(f"{'TVAE (æ¡ä»¶é‡‡æ ·)':<30} {f1_tvae:<12.4f} {ba_tvae:<15.4f}", flush=True)
print(f"{'CTGAN (æ‹’ç»é‡‡æ ·)':<30} {f1_ctgan_rej:<12.4f} {ba_ctgan_rej:<15.4f}", flush=True)
print(f"{'TVAE (æ‹’ç»é‡‡æ ·)':<30} {f1_tvae_rej:<12.4f} {ba_tvae_rej:<15.4f}", flush=True)

print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:", flush=True)
print(f"   - {SAVE_DIR}/Travel_CTGAN_minority_only.csv", flush=True)
print(f"   - {SAVE_DIR}/Travel_TVAE_minority_only.csv", flush=True)
print(f"   - {SAVE_DIR}/Travel_CTGAN_rejection_sampling.csv", flush=True)
print(f"   - {SAVE_DIR}/Travel_TVAE_rejection_sampling.csv", flush=True)

print("\nğŸ‰ å®Œæˆï¼æ‰€æœ‰æ–¹æ³•éƒ½åªç”Ÿæˆäº†å°‘æ•°ç±»æ ·æœ¬æ¥å¹³è¡¡æ•°æ®é›†ã€‚", flush=True)
print("=" * 70, flush=True)

