"""
Travel æ•°æ®é›†è¯„ä¼°è„šæœ¬
ä½œç”¨ï¼šè¯„ä¼° EPIC ç”Ÿæˆçš„åˆæˆæ•°æ®åœ¨ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½
"""
import os
import numpy as np
import pandas as pd
from tqdm.auto import tqdm
from tabulate import tabulate
import warnings
warnings.filterwarnings('ignore')

from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier

from xgboost import XGBClassifier
from catboost import CatBoostClassifier
import lightgbm as lgb

# å¯¼å…¥åŸæœ‰çš„å·¥å…·å‡½æ•°
import sys
sys.path.append('..')
from Classification import compute_metric, categorical_variable_encode, get_data, init_models

# ==========================================
# ğŸ“Š é…ç½®å‚æ•°
# ==========================================
DATA_NAME = 'travel'  # æ”¹ä¸ºå°å†™ï¼ŒåŒ¹é…å®é™…æ–‡ä»¶å¤¹å’Œæ–‡ä»¶å
synSamplingIndex = 0  # å¦‚æœç”Ÿæˆäº†å¤šæ‰¹æ•°æ®ï¼Œå¯ä»¥é€‰æ‹©ç¬¬å‡ æ‰¹
n = 1000  # ä½¿ç”¨å¤šå°‘æ¡åˆæˆæ•°æ®

# è¦æµ‹è¯•çš„åˆæˆæ–¹æ³•åˆ—è¡¨
SYNTHETIC_METHODS = [
    'None',  # åªç”¨çœŸå®æ•°æ®ï¼ˆåŸºçº¿ï¼‰
    'Travel_DeepSeek_EPIC',  # EPIC æ–¹æ³•
    # 'Travel_DeepSeek_EPICNorg',  # EPIC + çœŸå®æ•°æ®æ··åˆï¼ˆæš‚æ—¶æ³¨é‡Šï¼Œå› ä¸ºè¿˜æ²¡ç”Ÿæˆæ··åˆæ•°æ®ï¼‰
    # 'SMOTENC',  # ä¼ ç»Ÿè¿‡é‡‡æ ·æ–¹æ³•
    # 'SMOTENCNorg',  # SMOTENC + çœŸå®æ•°æ®æ··åˆ
]

# ==========================================
# ğŸ¯ æ•°æ®é›†é…ç½®
# ==========================================
DATA2TARGET = {
    'income':'income',
    'Diabetes':'readmitted',
    'HELOC':'RiskPerformance',
    'Sick':'Class',
    'Travel':'Target',
    'travel':'Target'  # æ·»åŠ å°å†™ç‰ˆæœ¬
}

DATA2NCLASS = {
    'income':2,
    'Diabetes':3,
    'HELOC':2,
    'Sick':2,
    'Travel':2,
    'travel':2  # æ·»åŠ å°å†™ç‰ˆæœ¬
}

ML_PARAMS = {
    'Travel':{
        'lr_max_iter':100,
        'dt_max_depth':6,
        'rf_max_depth':12,
        'rf_n_estimators':75,
    },
    'travel':{  # æ·»åŠ å°å†™ç‰ˆæœ¬
        'lr_max_iter':100,
        'dt_max_depth':6,
        'rf_max_depth':12,
        'rf_n_estimators':75,
    },
}

# ==========================================
# ğŸš€ å¼€å§‹è¯„ä¼°
# ==========================================
df_all_result = pd.DataFrame()

print("="*60)
print(f"ğŸ¯ å¼€å§‹è¯„ä¼° {DATA_NAME} æ•°æ®é›†")
print("="*60)

for sM in SYNTHETIC_METHODS:
    print(f"\nğŸ“Š æ­£åœ¨æµ‹è¯•æ–¹æ³•: {sM}")
    print("-"*60)
    
    configs={
        'data': DATA_NAME,
        'target': DATA2TARGET[DATA_NAME],
        'n_class': DATA2NCLASS[DATA_NAME],
        'is_regression': False,

        # hyperparams
        'lr_max_iter': ML_PARAMS[DATA_NAME]['lr_max_iter'],
        'dt_max_depth': ML_PARAMS[DATA_NAME]['dt_max_depth'],
        'rf_max_depth': ML_PARAMS[DATA_NAME]['rf_max_depth'],
        'rf_n_estimators': ML_PARAMS[DATA_NAME]['rf_n_estimators'],

        # xgboost
        'xg_max_depth': 4,
        'xg_lr': 0.03,

        # catboost
        'cat_max_depth': 6,
        'cat_lr': 0.003,

        # lightGBM
        'lgbm_max_depth': 3,
        'lgbm_lr': 0.1,

        # synthetic data
        'synModel': sM,
        'synSamples': n,
        'synSamplingIndex': synSamplingIndex,
        'cat_idx': [1, 2, 4, 5],  # Travel çš„åˆ†ç±»ç‰¹å¾ç´¢å¼•
    }
    
    models = init_models(configs, 42)
    syn_data_save_dir = f"../../data/syndata/{configs['synModel']}"
    real_data_save_dir = f"../../data/realdata/{configs['data']}"

    for k in tqdm(models.keys(), desc=f"Testing {sM}"):
        configs['model'] = k    
        for random_state in range(5):  # 5 æ¬¡éšæœºç§å­
            configs['random_state'] = random_state
            
            try:
                X_train, y_train, X_test, y_test, n_syn, n_org_train, n_org_test = get_data(
                    configs, syn_data_save_dir, real_data_save_dir
                )
            except Exception as e:
                print(f"âš ï¸ æ•°æ®åŠ è½½å¤±è´¥: {e}")
                continue
                
            df_save = pd.DataFrame([configs])
            df_save['n_syn'] = n_syn
            df_save['n_org_train'] = n_org_train
            df_save['n_org_test'] = n_org_test                

            model = init_models(configs, random_state)[k]

            scaler = StandardScaler()
            scaler.fit(X_train)

            X_train_np = scaler.transform(X_train)
            X_test_np = scaler.transform(X_test)

            model.fit(X_train_np, y_train)
            pred_test = model.predict(X_test_np)
            pred_test_proba = model.predict_proba(X_test_np)

            df_metric = compute_metric(y_test, pred_test, pred_test_proba, configs['n_class'], regression=configs['is_regression'])

            df_save = pd.concat([df_save, df_metric], axis=1)
            df_all_result = pd.concat([df_all_result, df_save.copy()])

# ==========================================
# ğŸ“Š ç»“æœæ±‡æ€»
# ==========================================
print("\n" + "="*60)
print("ğŸ“Š æœ€ç»ˆç»“æœæ±‡æ€»")
print("="*60)

df = df_all_result[['data','model','synModel','F1','BalancedACC','AUC']]
df = (df.groupby(['data','model','synModel']).mean()*100).reset_index()

print(tabulate(df, headers='keys', tablefmt='psql', floatfmt=".2f"))

# ä¿å­˜ç»“æœ
output_file = f"../../results/Travel_EPIC_results.csv"
os.makedirs(os.path.dirname(output_file), exist_ok=True)
df_all_result.to_csv(output_file, index=False)
print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

