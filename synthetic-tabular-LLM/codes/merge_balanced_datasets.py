"""
åˆå¹¶åŸå§‹è®­ç»ƒæ•°æ®å’Œç”Ÿæˆçš„å°‘æ•°ç±»æ ·æœ¬ï¼Œåˆ›å»ºå¹³è¡¡æ•°æ®é›†

ä¸º Sick, Travel, HELOC ä¸‰ä¸ªæ•°æ®é›†åˆ†åˆ«åˆå¹¶ï¼š
1. åŸå§‹è®­ç»ƒæ•°æ® + CTGANæ¡ä»¶é‡‡æ ·å°‘æ•°ç±» â†’ å¹³è¡¡æ•°æ®é›†
2. åŸå§‹è®­ç»ƒæ•°æ® + TVAEæ¡ä»¶é‡‡æ ·å°‘æ•°ç±» â†’ å¹³è¡¡æ•°æ®é›†
3. åŸå§‹è®­ç»ƒæ•°æ® + CTGANæ‹’ç»é‡‡æ ·å°‘æ•°ç±» â†’ å¹³è¡¡æ•°æ®é›†
4. åŸå§‹è®­ç»ƒæ•°æ® + TVAEæ‹’ç»é‡‡æ ·å°‘æ•°ç±» â†’ å¹³è¡¡æ•°æ®é›†

æ‰€æœ‰å¹³è¡¡æ•°æ®é›†ä¿å­˜åˆ°æ–°æ–‡ä»¶å¤¹: synthetic-tabular-LLM/data/balanced_datasets/
"""

import pandas as pd
import os
from pathlib import Path

# é…ç½®è·¯å¾„
BASE_DIR = Path('../data')
REALDATA_DIR = BASE_DIR / 'realdata'
SYNDATA_DIR = BASE_DIR / 'syndata'
OUTPUT_DIR = BASE_DIR / 'balanced_datasets'

# åˆ›å»ºè¾“å‡ºç›®å½•
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# æ•°æ®é›†é…ç½®
DATASETS_CONFIG = {
    'Sick': {
        'X_train': REALDATA_DIR / 'Sick' / 'X_train.csv',
        'y_train': REALDATA_DIR / 'Sick' / 'y_train.csv',
        'target_column': 'Class',
        'minority_class': 'sick',
        'majority_class': 'negative',
        'ctgan_conditional': SYNDATA_DIR / 'Sick_CTGAN_minority_only.csv',
        'tvae_conditional': SYNDATA_DIR / 'Sick_TVAE_minority_only.csv',
        'ctgan_rejection': SYNDATA_DIR / 'Sick_CTGAN_rejection_sampling.csv',
        'tvae_rejection': SYNDATA_DIR / 'Sick_TVAE_rejection_sampling.csv',
    },
    'Travel': {
        'X_train': REALDATA_DIR / 'travel' / 'X_train.csv',
        'y_train': REALDATA_DIR / 'travel' / 'y_train.csv',
        'target_column': 'Target',
        'minority_class': 1,
        'majority_class': 0,
        'ctgan_conditional': SYNDATA_DIR / 'Travel_CTGAN_minority_only.csv',
        'tvae_conditional': SYNDATA_DIR / 'Travel_TVAE_minority_only.csv',
        'ctgan_rejection': SYNDATA_DIR / 'Travel_CTGAN_rejection_sampling.csv',
        'tvae_rejection': SYNDATA_DIR / 'Travel_TVAE_rejection_sampling.csv',
    },
    'HELOC': {
        'X_train': REALDATA_DIR / 'HELOC' / 'X_train.csv',
        'y_train': REALDATA_DIR / 'HELOC' / 'y_train.csv',
        'target_column': 'RiskPerformance',
        'minority_class': 'RIS_1',
        'majority_class': 'RIS_0',
        'ctgan_conditional': SYNDATA_DIR / 'HELOC_CTGAN_minority_only.csv',
        'tvae_conditional': SYNDATA_DIR / 'HELOC_TVAE_minority_only.csv',
        'ctgan_rejection': SYNDATA_DIR / 'HELOC_CTGAN_rejection_sampling.csv',
        'tvae_rejection': SYNDATA_DIR / 'HELOC_TVAE_rejection_sampling.csv',
    }
}

def load_original_data(dataset_name, config):
    """åŠ è½½åŸå§‹è®­ç»ƒæ•°æ®"""
    print(f"\n{'='*60}")
    print(f"ğŸ“‚ åŠ è½½ {dataset_name} åŸå§‹è®­ç»ƒæ•°æ®...")
    
    X_train = pd.read_csv(config['X_train'])
    y_train = pd.read_csv(config['y_train'])
    
    # ç§»é™¤å¯èƒ½å­˜åœ¨çš„ index åˆ—
    if 'index' in y_train.columns:
        y_train = y_train.drop('index', axis=1)
    
    # åˆå¹¶ X å’Œ y
    target_column = config['target_column']
    train_data = pd.concat([X_train, y_train], axis=1)
    
    print(f"âœ… åŸå§‹è®­ç»ƒæ•°æ®: {len(train_data)} æ¡")
    print(f"   ç±»åˆ«åˆ†å¸ƒ:")
    print(train_data[target_column].value_counts())
    
    return train_data, target_column

def merge_and_balance(original_data, synthetic_data, target_column, minority_class, dataset_name, method_name):
    """åˆå¹¶åŸå§‹æ•°æ®å’Œåˆæˆå°‘æ•°ç±»æ•°æ®ï¼Œåˆ›å»ºå¹³è¡¡æ•°æ®é›†"""
    
    # ç¡®ä¿åˆæˆæ•°æ®åªåŒ…å«å°‘æ•°ç±»
    minority_syn = synthetic_data[synthetic_data[target_column] == minority_class].copy()
    
    print(f"\n   ğŸ“Š {method_name}:")
    print(f"      - åˆæˆå°‘æ•°ç±»æ ·æœ¬: {len(minority_syn)} æ¡")
    
    # åˆå¹¶æ•°æ®
    balanced_data = pd.concat([original_data, minority_syn], axis=0, ignore_index=True)
    
    # æ£€æŸ¥å¹³è¡¡æ€§
    class_counts = balanced_data[target_column].value_counts()
    print(f"      - åˆå¹¶åæ€»æ ·æœ¬: {len(balanced_data)} æ¡")
    print(f"      - ç±»åˆ«åˆ†å¸ƒ:")
    for cls, count in class_counts.items():
        percentage = count / len(balanced_data) * 100
        print(f"        {cls}: {count} ({percentage:.1f}%)")
    
    # è®¡ç®—ä¸å¹³è¡¡æ¯”ä¾‹
    max_count = class_counts.max()
    min_count = class_counts.min()
    imbalance_ratio = max_count / min_count
    print(f"      - ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}:1")
    
    if imbalance_ratio < 1.1:
        print(f"      âœ… æ•°æ®é›†æ¥è¿‘å®Œç¾å¹³è¡¡ï¼")
    elif imbalance_ratio < 1.5:
        print(f"      âœ… æ•°æ®é›†è½»åº¦ä¸å¹³è¡¡")
    else:
        print(f"      âš ï¸ æ•°æ®é›†ä»æœ‰ä¸å¹³è¡¡")
    
    return balanced_data

def process_dataset(dataset_name, config):
    """å¤„ç†å•ä¸ªæ•°æ®é›†"""
    print(f"\n{'#'*60}")
    print(f"# å¤„ç†æ•°æ®é›†: {dataset_name}")
    print(f"{'#'*60}")
    
    # åŠ è½½åŸå§‹æ•°æ®
    original_data, target_column = load_original_data(dataset_name, config)
    minority_class = config['minority_class']
    
    # åˆ›å»ºæ•°æ®é›†ä¸“å±æ–‡ä»¶å¤¹
    dataset_output_dir = OUTPUT_DIR / dataset_name
    dataset_output_dir.mkdir(parents=True, exist_ok=True)
    
    # å¤„ç†å››ç§åˆæˆæ–¹æ³•
    methods = [
        ('CTGAN_conditional', config['ctgan_conditional'], 'CTGANæ¡ä»¶é‡‡æ ·'),
        ('TVAE_conditional', config['tvae_conditional'], 'TVAEæ¡ä»¶é‡‡æ ·'),
        ('CTGAN_rejection', config['ctgan_rejection'], 'CTGANæ‹’ç»é‡‡æ ·'),
        ('TVAE_rejection', config['tvae_rejection'], 'TVAEæ‹’ç»é‡‡æ ·'),
    ]
    
    results = []
    
    for method_key, syn_file, method_name in methods:
        if not syn_file.exists():
            print(f"\n   âš ï¸ {method_name}: æ–‡ä»¶ä¸å­˜åœ¨ - {syn_file}")
            continue
        
        # åŠ è½½åˆæˆæ•°æ®
        synthetic_data = pd.read_csv(syn_file)
        
        # åˆå¹¶å¹¶å¹³è¡¡
        balanced_data = merge_and_balance(
            original_data, synthetic_data, target_column, 
            minority_class, dataset_name, method_name
        )
        
        # ä¿å­˜å¹³è¡¡æ•°æ®é›†
        output_file = dataset_output_dir / f'{dataset_name}_balanced_{method_key}.csv'
        balanced_data.to_csv(output_file, index=False)
        print(f"      ğŸ’¾ å·²ä¿å­˜: {output_file.name}")
        
        results.append({
            'method': method_name,
            'file': output_file.name,
            'samples': len(balanced_data)
        })
    
    return results

# ä¸»ç¨‹åº
if __name__ == '__main__':
    print("="*60)
    print("ğŸš€ å¼€å§‹åˆå¹¶åŸå§‹æ•°æ®å’Œåˆæˆå°‘æ•°ç±»æ•°æ®ï¼Œåˆ›å»ºå¹³è¡¡æ•°æ®é›†")
    print("="*60)
    
    all_results = {}
    
    for dataset_name, config in DATASETS_CONFIG.items():
        results = process_dataset(dataset_name, config)
        all_results[dataset_name] = results
    
    # æœ€ç»ˆæ€»ç»“
    print(f"\n{'='*60}")
    print("ğŸ‰ æ‰€æœ‰æ•°æ®é›†å¤„ç†å®Œæˆï¼")
    print(f"{'='*60}")
    
    print(f"\nğŸ“ æ‰€æœ‰å¹³è¡¡æ•°æ®é›†å·²ä¿å­˜åˆ°: {OUTPUT_DIR}")
    print(f"\nğŸ“Š ç”Ÿæˆçš„å¹³è¡¡æ•°æ®é›†æ€»ç»“:\n")
    
    for dataset_name, results in all_results.items():
        print(f"  {dataset_name}:")
        for result in results:
            print(f"    âœ… {result['file']} ({result['samples']} æ¡)")
    
    print(f"\nâœ¨ å®Œæˆï¼å…±ç”Ÿæˆ {sum(len(r) for r in all_results.values())} ä¸ªå¹³è¡¡æ•°æ®é›†ï¼")

