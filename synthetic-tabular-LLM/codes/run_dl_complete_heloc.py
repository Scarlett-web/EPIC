"""
HELOC æ•°æ®é›†å®Œæ•´æ·±åº¦å­¦ä¹ å®éªŒ
åŒ…æ‹¬ï¼š
1. æ ‡å‡†ç”Ÿæˆï¼ˆä¿æŒåŸå§‹ä¸å¹³è¡¡åˆ†å¸ƒï¼‰- CTGAN + TVAE
2. æ¡ä»¶é‡‡æ ·ï¼ˆåªç”Ÿæˆå°‘æ•°ç±»ï¼‰- CTGAN + TVAE
3. æ‹’ç»é‡‡æ ·ï¼ˆåªç”Ÿæˆå°‘æ•°ç±»ï¼‰- CTGAN + TVAE
ä¸€å…±ç”Ÿæˆ 6 ä¸ªæ•°æ®é›†
"""
import sys
import pandas as pd
import numpy as np
from sdv.single_table import CTGANSynthesizer, TVAESynthesizer
from sdv.metadata import SingleTableMetadata
from sdv.sampling import Condition
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier
from sklearn.metrics import f1_score, balanced_accuracy_score, classification_report
import time
import warnings
warnings.filterwarnings('ignore')

# é…ç½®è¾“å‡ºç¼“å†²
sys.stdout.reconfigure(line_buffering=True)

# æ•°æ®é›†é…ç½®
DATASET_NAME = 'HELOC'
TARGET_COLUMN = 'RiskPerformance'
MINORITY_CLASS = 'RIS_1'  # å°‘æ•°ç±»
MAJORITY_CLASS = 'RIS_0'  # å¤šæ•°ç±»
N_SAMPLES_STANDARD = 1000  # æ ‡å‡†ç”Ÿæˆçš„æ ·æœ¬æ•°

print("=" * 80, flush=True)
print("ğŸš€ HELOC æ•°æ®é›†å®Œæ•´æ·±åº¦å­¦ä¹ å®éªŒ", flush=True)
print("=" * 80, flush=True)

# ============================================================================
# 1. åŠ è½½æ•°æ®
# ============================================================================
print("\nğŸ“‚ åŠ è½½æ•°æ®...", flush=True)
X_train = pd.read_csv('../data/realdata/HELOC/X_train.csv')
y_train = pd.read_csv('../data/realdata/HELOC/y_train.csv')
X_test = pd.read_csv('../data/realdata/HELOC/X_test.csv')
y_test = pd.read_csv('../data/realdata/HELOC/y_test.csv')

# åˆ é™¤ index åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
if 'index' in X_train.columns:
    X_train = X_train.drop('index', axis=1)
if 'index' in X_test.columns:
    X_test = X_test.drop('index', axis=1)
if 'index' in y_train.columns:
    y_train = y_train.drop('index', axis=1)
if 'index' in y_test.columns:
    y_test = y_test.drop('index', axis=1)

# åˆå¹¶ X å’Œ y
train_data = pd.concat([X_train, y_train], axis=1)

print(f"âœ… è®­ç»ƒé›†: {train_data.shape}", flush=True)
print(f"âœ… æµ‹è¯•é›†: {X_test.shape}", flush=True)

# ç±»åˆ«åˆ†å¸ƒ
class_counts = y_train[TARGET_COLUMN].value_counts()
print(f"\nğŸ“Š åŸå§‹ç±»åˆ«åˆ†å¸ƒ:", flush=True)
for cls, count in class_counts.items():
    print(f"   {cls}: {count} ({count/len(y_train)*100:.2f}%)", flush=True)

minority_count = class_counts[MINORITY_CLASS]
majority_count = class_counts[MAJORITY_CLASS]
samples_needed = int(majority_count - minority_count)

print(f"\nğŸ¯ éœ€è¦ç”Ÿæˆ {samples_needed} æ¡å°‘æ•°ç±»æ ·æœ¬æ¥å¹³è¡¡æ•°æ®é›†", flush=True)

# ============================================================================
# 2. å‡†å¤‡å…ƒæ•°æ®
# ============================================================================
print("\nğŸ”§ å‡†å¤‡å…ƒæ•°æ®...", flush=True)
metadata = SingleTableMetadata()
metadata.detect_from_dataframe(train_data)
print("âœ… å…ƒæ•°æ®å‡†å¤‡å®Œæˆ", flush=True)

# ============================================================================
# 3. è¯„ä¼°å‡½æ•°
# ============================================================================
def evaluate_model(name, X_train_eval, y_train_eval, X_test_eval, y_test_eval):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print(f"\n{'='*60}", flush=True)
    print(f"ğŸ“Š è¯„ä¼°: {name}", flush=True)
    print(f"{'='*60}", flush=True)
    
    # ç¼–ç åˆ†ç±»ç‰¹å¾
    label_encoders = {}
    X_train_encoded = X_train_eval.copy()
    X_test_encoded = X_test_eval.copy()
    
    for col in X_train_encoded.columns:
        if X_train_encoded[col].dtype == 'object':
            le = LabelEncoder()
            X_train_encoded[col] = le.fit_transform(X_train_encoded[col].astype(str))
            X_test_encoded[col] = le.transform(X_test_encoded[col].astype(str))
            label_encoders[col] = le
    
    # ç¼–ç ç›®æ ‡å˜é‡
    le_target = LabelEncoder()
    y_train_encoded = le_target.fit_transform(y_train_eval[TARGET_COLUMN])
    y_test_encoded = le_target.transform(y_test_eval[TARGET_COLUMN])
    
    # è®­ç»ƒ XGBoost
    start_time = time.time()
    model = XGBClassifier(random_state=42, eval_metric='logloss')
    model.fit(X_train_encoded, y_train_encoded)
    train_time = (time.time() - start_time) / 60
    
    # é¢„æµ‹
    y_pred = model.predict(X_test_encoded)
    
    # è®¡ç®—æŒ‡æ ‡
    f1 = f1_score(y_test_encoded, y_pred, average='weighted')
    ba = balanced_accuracy_score(y_test_encoded, y_pred)
    
    print(f"âœ… F1 Score: {f1:.4f}", flush=True)
    print(f"âœ… Balanced Accuracy: {ba:.4f}", flush=True)
    print(f"â±ï¸  è®­ç»ƒæ—¶é—´: {train_time:.2f} åˆ†é’Ÿ", flush=True)
    
    print(f"\nåˆ†ç±»æŠ¥å‘Š:", flush=True)
    print(classification_report(y_test_encoded, y_pred, 
                                target_names=le_target.classes_), flush=True)
    
    return f1, ba, train_time

# ============================================================================
# 4. å®éªŒ 1: CTGAN æ ‡å‡†ç”Ÿæˆï¼ˆä¿æŒä¸å¹³è¡¡ï¼‰
# ============================================================================
print("\n" + "="*80, flush=True)
print("ğŸ”µ å®éªŒ 1/6: CTGAN æ ‡å‡†ç”Ÿæˆï¼ˆä¿æŒåŸå§‹ä¸å¹³è¡¡åˆ†å¸ƒï¼‰", flush=True)
print("="*80, flush=True)

print("\nğŸ‹ï¸  è®­ç»ƒ CTGAN...", flush=True)
ctgan = CTGANSynthesizer(metadata, epochs=300, verbose=True)
ctgan.fit(train_data)
print("âœ… CTGAN è®­ç»ƒå®Œæˆ", flush=True)

print(f"\nğŸ² ç”Ÿæˆ {N_SAMPLES_STANDARD} æ¡æ ·æœ¬...", flush=True)
syn_ctgan_standard = ctgan.sample(num_rows=N_SAMPLES_STANDARD)
print(f"âœ… ç”Ÿæˆå®Œæˆ: {syn_ctgan_standard.shape}", flush=True)
print(f"   ç±»åˆ«åˆ†å¸ƒ: {syn_ctgan_standard[TARGET_COLUMN].value_counts().to_dict()}", flush=True)

# ä¿å­˜
output_path = f'../data/syndata/HELOC_CTGAN_standard.csv'
syn_ctgan_standard.to_csv(output_path, index=False)
print(f"ğŸ’¾ å·²ä¿å­˜: {output_path}", flush=True)

# è¯„ä¼°
X_syn = syn_ctgan_standard.drop(TARGET_COLUMN, axis=1)
y_syn = syn_ctgan_standard[[TARGET_COLUMN]]
X_combined = pd.concat([X_train, X_syn], axis=0)
y_combined = pd.concat([y_train, y_syn], axis=0)
f1_ctgan_std, ba_ctgan_std, time_ctgan_std = evaluate_model(
    "CTGAN æ ‡å‡†ç”Ÿæˆ", X_combined, y_combined, X_test, y_test
)

# ============================================================================
# 5. å®éªŒ 2: TVAE æ ‡å‡†ç”Ÿæˆï¼ˆä¿æŒä¸å¹³è¡¡ï¼‰
# ============================================================================
print("\n" + "="*80, flush=True)
print("ğŸŸ¢ å®éªŒ 2/6: TVAE æ ‡å‡†ç”Ÿæˆï¼ˆä¿æŒåŸå§‹ä¸å¹³è¡¡åˆ†å¸ƒï¼‰", flush=True)
print("="*80, flush=True)

print("\nğŸ‹ï¸  è®­ç»ƒ TVAE...", flush=True)
tvae = TVAESynthesizer(metadata, epochs=300, verbose=True)
tvae.fit(train_data)
print("âœ… TVAE è®­ç»ƒå®Œæˆ", flush=True)

print(f"\nğŸ² ç”Ÿæˆ {N_SAMPLES_STANDARD} æ¡æ ·æœ¬...", flush=True)
syn_tvae_standard = tvae.sample(num_rows=N_SAMPLES_STANDARD)
print(f"âœ… ç”Ÿæˆå®Œæˆ: {syn_tvae_standard.shape}", flush=True)
print(f"   ç±»åˆ«åˆ†å¸ƒ: {syn_tvae_standard[TARGET_COLUMN].value_counts().to_dict()}", flush=True)

# ä¿å­˜
output_path = f'../data/syndata/HELOC_TVAE_standard.csv'
syn_tvae_standard.to_csv(output_path, index=False)
print(f"ğŸ’¾ å·²ä¿å­˜: {output_path}", flush=True)

# è¯„ä¼°
X_syn = syn_tvae_standard.drop(TARGET_COLUMN, axis=1)
y_syn = syn_tvae_standard[[TARGET_COLUMN]]
X_combined = pd.concat([X_train, X_syn], axis=0)
y_combined = pd.concat([y_train, y_syn], axis=0)
f1_tvae_std, ba_tvae_std, time_tvae_std = evaluate_model(
    "TVAE æ ‡å‡†ç”Ÿæˆ", X_combined, y_combined, X_test, y_test
)

# ============================================================================
# 6. å®éªŒ 3: CTGAN æ¡ä»¶é‡‡æ ·ï¼ˆåªç”Ÿæˆå°‘æ•°ç±»ï¼‰
# ============================================================================
print("\n" + "="*80, flush=True)
print("ğŸŸ£ å®éªŒ 3/6: CTGAN æ¡ä»¶é‡‡æ ·ï¼ˆåªç”Ÿæˆå°‘æ•°ç±»ï¼‰", flush=True)
print("="*80, flush=True)

print(f"\nğŸ¯ ä½¿ç”¨æ¡ä»¶é‡‡æ ·ç”Ÿæˆ {samples_needed} æ¡ {MINORITY_CLASS} æ ·æœ¬...", flush=True)
condition = Condition(num_rows=samples_needed, column_values={TARGET_COLUMN: MINORITY_CLASS})
syn_ctgan_conditional = ctgan.sample_from_conditions(conditions=[condition])
print(f"âœ… ç”Ÿæˆå®Œæˆ: {syn_ctgan_conditional.shape}", flush=True)
print(f"   ç±»åˆ«åˆ†å¸ƒ: {syn_ctgan_conditional[TARGET_COLUMN].value_counts().to_dict()}", flush=True)

# ä¿å­˜
output_path = f'../data/syndata/HELOC_CTGAN_minority_only.csv'
syn_ctgan_conditional.to_csv(output_path, index=False)
print(f"ğŸ’¾ å·²ä¿å­˜: {output_path}", flush=True)

# è¯„ä¼°
X_syn = syn_ctgan_conditional.drop(TARGET_COLUMN, axis=1)
y_syn = syn_ctgan_conditional[[TARGET_COLUMN]]
X_balanced = pd.concat([X_train, X_syn], axis=0)
y_balanced = pd.concat([y_train, y_syn], axis=0)

print(f"\nğŸ“Š å¹³è¡¡åçš„ç±»åˆ«åˆ†å¸ƒ:", flush=True)
for cls, count in y_balanced[TARGET_COLUMN].value_counts().items():
    print(f"   {cls}: {count} ({count/len(y_balanced)*100:.2f}%)", flush=True)

f1_ctgan_cond, ba_ctgan_cond, time_ctgan_cond = evaluate_model(
    "CTGAN æ¡ä»¶é‡‡æ ·", X_balanced, y_balanced, X_test, y_test
)

# ============================================================================
# 7. å®éªŒ 4: TVAE æ¡ä»¶é‡‡æ ·ï¼ˆåªç”Ÿæˆå°‘æ•°ç±»ï¼‰
# ============================================================================
print("\n" + "="*80, flush=True)
print("ğŸŸ¡ å®éªŒ 4/6: TVAE æ¡ä»¶é‡‡æ ·ï¼ˆåªç”Ÿæˆå°‘æ•°ç±»ï¼‰", flush=True)
print("="*80, flush=True)

print(f"\nğŸ¯ ä½¿ç”¨æ¡ä»¶é‡‡æ ·ç”Ÿæˆ {samples_needed} æ¡ {MINORITY_CLASS} æ ·æœ¬...", flush=True)
condition = Condition(num_rows=samples_needed, column_values={TARGET_COLUMN: MINORITY_CLASS})
syn_tvae_conditional = tvae.sample_from_conditions(conditions=[condition])
print(f"âœ… ç”Ÿæˆå®Œæˆ: {syn_tvae_conditional.shape}", flush=True)
print(f"   ç±»åˆ«åˆ†å¸ƒ: {syn_tvae_conditional[TARGET_COLUMN].value_counts().to_dict()}", flush=True)

# ä¿å­˜
output_path = f'../data/syndata/HELOC_TVAE_minority_only.csv'
syn_tvae_conditional.to_csv(output_path, index=False)
print(f"ğŸ’¾ å·²ä¿å­˜: {output_path}", flush=True)

# è¯„ä¼°
X_syn = syn_tvae_conditional.drop(TARGET_COLUMN, axis=1)
y_syn = syn_tvae_conditional[[TARGET_COLUMN]]
X_balanced = pd.concat([X_train, X_syn], axis=0)
y_balanced = pd.concat([y_train, y_syn], axis=0)

print(f"\nğŸ“Š å¹³è¡¡åçš„ç±»åˆ«åˆ†å¸ƒ:", flush=True)
for cls, count in y_balanced[TARGET_COLUMN].value_counts().items():
    print(f"   {cls}: {count} ({count/len(y_balanced)*100:.2f}%)", flush=True)

f1_tvae_cond, ba_tvae_cond, time_tvae_cond = evaluate_model(
    "TVAE æ¡ä»¶é‡‡æ ·", X_balanced, y_balanced, X_test, y_test
)

# ============================================================================
# 8. å®éªŒ 5: CTGAN æ‹’ç»é‡‡æ ·ï¼ˆåªç”Ÿæˆå°‘æ•°ç±»ï¼‰
# ============================================================================
print("\n" + "="*80, flush=True)
print("ğŸ”´ å®éªŒ 5/6: CTGAN æ‹’ç»é‡‡æ ·ï¼ˆåªç”Ÿæˆå°‘æ•°ç±»ï¼‰", flush=True)
print("="*80, flush=True)

oversample_factor = 5
total_samples = samples_needed * oversample_factor
print(f"\nğŸ² ç”Ÿæˆ {total_samples} æ¡æ ·æœ¬ï¼ˆ{oversample_factor}x è¿‡é‡‡æ ·ï¼‰...", flush=True)
syn_all = ctgan.sample(num_rows=total_samples)
print(f"âœ… ç”Ÿæˆå®Œæˆ", flush=True)

print(f"\nğŸ” ç­›é€‰å°‘æ•°ç±»æ ·æœ¬...", flush=True)
syn_minority = syn_all[syn_all[TARGET_COLUMN] == MINORITY_CLASS]
print(f"   ç­›é€‰å‡º {len(syn_minority)} æ¡å°‘æ•°ç±»æ ·æœ¬ï¼ˆæ•ˆç‡: {len(syn_minority)/total_samples*100:.1f}%ï¼‰", flush=True)

if len(syn_minority) >= samples_needed:
    syn_ctgan_rejection = syn_minority.head(samples_needed)
    print(f"âœ… ä¿ç•™ {samples_needed} æ¡æ ·æœ¬", flush=True)
else:
    print(f"âš ï¸  æ ·æœ¬ä¸è¶³ï¼Œåªæœ‰ {len(syn_minority)} æ¡ï¼ˆéœ€è¦ {samples_needed} æ¡ï¼‰", flush=True)
    syn_ctgan_rejection = syn_minority

print(f"   æœ€ç»ˆæ ·æœ¬æ•°: {syn_ctgan_rejection.shape}", flush=True)
print(f"   ç±»åˆ«åˆ†å¸ƒ: {syn_ctgan_rejection[TARGET_COLUMN].value_counts().to_dict()}", flush=True)

# ä¿å­˜
output_path = f'../data/syndata/HELOC_CTGAN_rejection_sampling.csv'
syn_ctgan_rejection.to_csv(output_path, index=False)
print(f"ğŸ’¾ å·²ä¿å­˜: {output_path}", flush=True)

# è¯„ä¼°
X_syn = syn_ctgan_rejection.drop(TARGET_COLUMN, axis=1)
y_syn = syn_ctgan_rejection[[TARGET_COLUMN]]
X_balanced = pd.concat([X_train, X_syn], axis=0)
y_balanced = pd.concat([y_train, y_syn], axis=0)

print(f"\nğŸ“Š å¹³è¡¡åçš„ç±»åˆ«åˆ†å¸ƒ:", flush=True)
for cls, count in y_balanced[TARGET_COLUMN].value_counts().items():
    print(f"   {cls}: {count} ({count/len(y_balanced)*100:.2f}%)", flush=True)

f1_ctgan_rej, ba_ctgan_rej, time_ctgan_rej = evaluate_model(
    "CTGAN æ‹’ç»é‡‡æ ·", X_balanced, y_balanced, X_test, y_test
)

# ============================================================================
# 9. å®éªŒ 6: TVAE æ‹’ç»é‡‡æ ·ï¼ˆåªç”Ÿæˆå°‘æ•°ç±»ï¼‰
# ============================================================================
print("\n" + "="*80, flush=True)
print("ğŸŸ  å®éªŒ 6/6: TVAE æ‹’ç»é‡‡æ ·ï¼ˆåªç”Ÿæˆå°‘æ•°ç±»ï¼‰", flush=True)
print("="*80, flush=True)

print(f"\nğŸ² ç”Ÿæˆ {total_samples} æ¡æ ·æœ¬ï¼ˆ{oversample_factor}x è¿‡é‡‡æ ·ï¼‰...", flush=True)
syn_all = tvae.sample(num_rows=total_samples)
print(f"âœ… ç”Ÿæˆå®Œæˆ", flush=True)

print(f"\nğŸ” ç­›é€‰å°‘æ•°ç±»æ ·æœ¬...", flush=True)
syn_minority = syn_all[syn_all[TARGET_COLUMN] == MINORITY_CLASS]
print(f"   ç­›é€‰å‡º {len(syn_minority)} æ¡å°‘æ•°ç±»æ ·æœ¬ï¼ˆæ•ˆç‡: {len(syn_minority)/total_samples*100:.1f}%ï¼‰", flush=True)

if len(syn_minority) >= samples_needed:
    syn_tvae_rejection = syn_minority.head(samples_needed)
    print(f"âœ… ä¿ç•™ {samples_needed} æ¡æ ·æœ¬", flush=True)
else:
    print(f"âš ï¸  æ ·æœ¬ä¸è¶³ï¼Œåªæœ‰ {len(syn_minority)} æ¡ï¼ˆéœ€è¦ {samples_needed} æ¡ï¼‰", flush=True)
    syn_tvae_rejection = syn_minority

print(f"   æœ€ç»ˆæ ·æœ¬æ•°: {syn_tvae_rejection.shape}", flush=True)
print(f"   ç±»åˆ«åˆ†å¸ƒ: {syn_tvae_rejection[TARGET_COLUMN].value_counts().to_dict()}", flush=True)

# ä¿å­˜
output_path = f'../data/syndata/HELOC_TVAE_rejection_sampling.csv'
syn_tvae_rejection.to_csv(output_path, index=False)
print(f"ğŸ’¾ å·²ä¿å­˜: {output_path}", flush=True)

# è¯„ä¼°
X_syn = syn_tvae_rejection.drop(TARGET_COLUMN, axis=1)
y_syn = syn_tvae_rejection[[TARGET_COLUMN]]
X_balanced = pd.concat([X_train, X_syn], axis=0)
y_balanced = pd.concat([y_train, y_syn], axis=0)

print(f"\nğŸ“Š å¹³è¡¡åçš„ç±»åˆ«åˆ†å¸ƒ:", flush=True)
for cls, count in y_balanced[TARGET_COLUMN].value_counts().items():
    print(f"   {cls}: {count} ({count/len(y_balanced)*100:.2f}%)", flush=True)

f1_tvae_rej, ba_tvae_rej, time_tvae_rej = evaluate_model(
    "TVAE æ‹’ç»é‡‡æ ·", X_balanced, y_balanced, X_test, y_test
)

# ============================================================================
# 10. æœ€ç»ˆæ€»ç»“
# ============================================================================
print("\n" + "="*80, flush=True)
print("ğŸ‰ å®éªŒå®Œæˆï¼æœ€ç»ˆç»“æœæ±‡æ€»", flush=True)
print("="*80, flush=True)

results = {
    'CTGAN æ ‡å‡†ç”Ÿæˆ': {'f1': f1_ctgan_std, 'ba': ba_ctgan_std, 'time': time_ctgan_std},
    'TVAE æ ‡å‡†ç”Ÿæˆ': {'f1': f1_tvae_std, 'ba': ba_tvae_std, 'time': time_tvae_std},
    'CTGAN æ¡ä»¶é‡‡æ ·': {'f1': f1_ctgan_cond, 'ba': ba_ctgan_cond, 'time': time_ctgan_cond},
    'TVAE æ¡ä»¶é‡‡æ ·': {'f1': f1_tvae_cond, 'ba': ba_tvae_cond, 'time': time_tvae_cond},
    'CTGAN æ‹’ç»é‡‡æ ·': {'f1': f1_ctgan_rej, 'ba': ba_ctgan_rej, 'time': time_ctgan_rej},
    'TVAE æ‹’ç»é‡‡æ ·': {'f1': f1_tvae_rej, 'ba': ba_tvae_rej, 'time': time_tvae_rej},
}

print("\nğŸ“Š æ€§èƒ½å¯¹æ¯”è¡¨æ ¼:")
print(f"{'æ–¹æ³•':<20} {'F1 Score':<12} {'Balanced Acc':<15} {'è®­ç»ƒæ—¶é—´(åˆ†é’Ÿ)':<15}")
print("-" * 65)
for method, metrics in results.items():
    print(f"{method:<20} {metrics['f1']:<12.4f} {metrics['ba']:<15.4f} {metrics['time']:<15.2f}")

# æ‰¾å‡ºæœ€ä½³æ–¹æ³•
best_f1_method = max(results.items(), key=lambda x: x[1]['f1'])
best_ba_method = max(results.items(), key=lambda x: x[1]['ba'])

print(f"\nğŸ† æœ€ä½³ F1 Score: {best_f1_method[0]} ({best_f1_method[1]['f1']:.4f})")
print(f"ğŸ† æœ€ä½³ Balanced Accuracy: {best_ba_method[0]} ({best_ba_method[1]['ba']:.4f})")

print("\nâœ… æ‰€æœ‰ 6 ä¸ªæ•°æ®é›†å·²ç”Ÿæˆå¹¶ä¿å­˜åˆ° ../data/syndata/")
print("="*80, flush=True)

