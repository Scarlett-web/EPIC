"""
HELOC æ•°æ®é›† EPIC ç”Ÿæˆè„šæœ¬
ä½œç”¨ï¼šä½¿ç”¨ DeepSeek-V3 ç”Ÿæˆ HELOC æ•°æ®é›†çš„åˆæˆæ ·æœ¬
"""
import time
import openai
import os
import pandas as pd
import string
import random
import httpx

from langchain_openai import ChatOpenAI 
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate

# å¼ºåˆ¶æ¸…é™¤ç³»ç»Ÿä»£ç†è®¾ç½®
os.environ.pop("HTTP_PROXY", None)
os.environ.pop("HTTPS_PROXY", None)
os.environ.pop("http_proxy", None)
os.environ.pop("https_proxy", None)

# ==========================================
# ğŸ”‘ API é…ç½®
# ==========================================
my_deepseek_key = "sk-erltuaebsxiimieebxdxlbeifvootbvnacyzmglozboutlyg"

# ==========================================
# ğŸ“Š å‚æ•°é…ç½®
# ==========================================
params = {
    "openai_key": my_deepseek_key,
    "model": "deepseek-ai/DeepSeek-V3",  # ä½¿ç”¨å®Œæ•´æ¨¡å‹åç§°
    "DATA_NAME": "HELOC",  # æ•°æ®é›†åç§°ï¼ˆå¤§å†™ï¼ŒåŒ¹é…æ–‡ä»¶å¤¹ï¼‰
    "TARGET": "RiskPerformance",  # ç›®æ ‡å˜é‡å
    "N_CLASS": 2,  # ç±»åˆ«æ•°
    "N_SAMPLES_PER_CLASS": 15,  # æ¯ç±»ç»™ 15 ä¸ªæ ·æœ¬ä½œä¸ºç¤ºä¾‹
    "N_SET": 4,
    "USE_RANDOM_WORD": True,  # ä½¿ç”¨éšæœºå•è¯æ˜ å°„
    "N_BATCH": 20,  # æ¯æ¬¡ç”Ÿæˆ 20 è¡Œ
    "MODEL_NAME": "HELOC_DeepSeek_EPIC",  # æ¨¡å‹åç§°
    "N_TARGET_SAMPLES": 1000,  # ç›®æ ‡ç”Ÿæˆ 1000 æ¡
}

params.update({
    "DATA_DIR": f"../../data/realdata/{params['DATA_NAME']}",
    "SAVE_DIR": f"../../data/syndata/{params['MODEL_NAME']}"
})

# ==========================================
# ğŸ”Œ åˆå§‹åŒ– API
# ==========================================
print("="*60)
print(f"ğŸš€ å¼€å§‹ç”Ÿæˆ {params['DATA_NAME']} æ•°æ®é›†çš„åˆæˆæ ·æœ¬")
print("="*60)

# åˆ›å»ºè‡ªå®šä¹‰ HTTP å®¢æˆ·ç«¯ï¼ˆç¦ç”¨ä»£ç†ï¼‰
http_client = httpx.Client(timeout=60.0)

llm = ChatOpenAI(
    model=params['model'],
    openai_api_key=params['openai_key'],
    openai_api_base="https://api.siliconflow.cn/v1",
    temperature=0.1,
    http_client=http_client
)

# ==========================================
# ğŸ“ Prompt æ¨¡æ¿
# ==========================================
initial_prompt = """
[SYSTEM INSTRUCTION]
You are a strict tabular data generator. Generate EXACTLY {n_batch} rows of synthetic data in CSV format.

[DATA DESCRIPTION]
This is HELOC (Home Equity Line of Credit) risk assessment data with the following features:

RiskPerformance: credit risk level (RIS_0 = Good, RIS_1 = Bad),
ExternalRiskEstimate: external risk score (0-100),
MSinceOldestTradeOpen: months since oldest trade opened,
MSinceMostRecentTradeOpen: months since most recent trade opened,
AverageMInFile: average months in file,
NumSatisfactoryTrades: number of satisfactory trades,
NumTrades60Ever2DerogPubRec: number of trades 60+ days past due,
NumTrades90Ever2DerogPubRec: number of trades 90+ days past due,
PercentTradesNeverDelq: percent of trades never delinquent,
MSinceMostRecentDelq: months since most recent delinquency,
MaxDelq2PublicRecLast12M: maximum delinquency in last 12 months,
MaxDelqEver: maximum delinquency ever,
NumTotalTrades: total number of trades,
NumTradesOpeninLast12M: number of trades opened in last 12 months,
PercentInstallTrades: percent of installment trades,
MSinceMostRecentInqexcl7days: months since most recent inquiry (excluding 7 days),
NumInqLast6M: number of inquiries in last 6 months,
NumInqLast6Mexcl7days: number of inquiries in last 6 months (excluding 7 days),
NetFractionRevolvingBurden: net fraction of revolving burden,
NetFractionInstallBurden: net fraction of installment burden,
NumRevolvingTradesWBalance: number of revolving trades with balance,
NumInstallTradesWBalance: number of installment trades with balance,
NumBank2NatlTradesWHighUtilization: number of bank/national trades with high utilization,
PercentTradesWBalance: percent of trades with balance

[EXAMPLES]
{examples}

[TASK]
Generate EXACTLY {n_batch} new rows following the same pattern. Output ONLY the CSV data with header, no explanations.
"""

# ==========================================
# ğŸ“‚ åŠ è½½æ•°æ®
# ==========================================
print(f"\nğŸ“‚ Loading data from {params['DATA_DIR']}...")
X_train = pd.read_csv(os.path.join(params['DATA_DIR'], 'X_train.csv'), index_col='index')
y_train = pd.read_csv(os.path.join(params['DATA_DIR'], 'y_train.csv'), index_col='index')

# åˆå¹¶ç‰¹å¾å’Œç›®æ ‡
train_data = pd.concat([y_train, X_train], axis=1)

print(f"   è®­ç»ƒæ•°æ®å½¢çŠ¶: {train_data.shape}")
print(f"   ç›®æ ‡å˜é‡åˆ†å¸ƒ:")
print(train_data[params['TARGET']].value_counts())

# ==========================================
# ğŸ”€ éšæœºå•è¯æ˜ å°„ï¼ˆRandom Word Mappingï¼‰
# ==========================================
# HELOC æ•°æ®é›†åªæœ‰ç›®æ ‡å˜é‡æ˜¯åˆ†ç±»çš„ï¼Œå…¶ä»–éƒ½æ˜¯æ•°å€¼
CATEGORICAL_FEATURES = ['RiskPerformance']

mapping_dict = {}
reverse_mapping_dict = {}

if params['USE_RANDOM_WORD']: 
    print(f"\nğŸ”€ Applying Unique Variable Mapping strategy...")
    
    for col in CATEGORICAL_FEATURES:
        if col in train_data.columns:
            unique_values = train_data[col].unique()
            random_codes = [f"{col[:3].upper()}_{i}" for i in range(len(unique_values))]
            
            mapping_dict[col] = dict(zip(unique_values, random_codes))
            reverse_mapping_dict[col] = dict(zip(random_codes, unique_values))
            
            train_data[col] = train_data[col].map(mapping_dict[col])
            
            print(f"   {col}: {mapping_dict[col]}")

# ==========================================
# ğŸ¯ Few-shot é‡‡æ ·
# ==========================================
print(f"\nğŸ¯ Sampling {params['N_SAMPLES_PER_CLASS']} examples per class...")

sampled_data = []
for class_label in train_data[params['TARGET']].unique():
    class_data = train_data[train_data[params['TARGET']] == class_label]
    sampled = class_data.sample(n=min(params['N_SAMPLES_PER_CLASS'], len(class_data)), random_state=42)
    sampled_data.append(sampled)

few_shot_examples = pd.concat(sampled_data, axis=0)
examples_csv = few_shot_examples.to_csv(index=False)

print(f"   Few-shot æ ·æœ¬æ•°: {len(few_shot_examples)}")

# ==========================================
# ğŸ”„ æ‰¹é‡ç”Ÿæˆ
# ==========================================
print(f"\nğŸ”„ Start generating {params['N_TARGET_SAMPLES']} samples...")

prompt_template = PromptTemplate(
    input_variables=["examples", "n_batch"],
    template=initial_prompt
)

chain = prompt_template | llm | StrOutputParser()

all_generated_samples = []
n_generated = 0
n_errors = 0

while n_generated < params['N_TARGET_SAMPLES']:
    try:
        # è°ƒç”¨ LLM
        response = chain.invoke({
            "examples": examples_csv,
            "n_batch": params['N_BATCH']
        })

        # è§£æ CSV
        from io import StringIO
        try:
            generated_df = pd.read_csv(StringIO(response))

            # éªŒè¯åˆ—å
            if set(generated_df.columns) == set(train_data.columns):
                all_generated_samples.append(generated_df)
                n_generated += len(generated_df)
                print(f"Progress: {n_generated} / {params['N_TARGET_SAMPLES']}")
            else:
                n_errors += 1
                print(f"âš ï¸  Column mismatch, skipping...")

        except Exception as e:
            n_errors += 1
            print(f"âš ï¸  Parsing error: {e}")

        # é¿å… API é™æµ
        if n_generated < params['N_TARGET_SAMPLES']:
            print(f"Sleeping for 60s to avoid rate limit...")
            time.sleep(60)

    except Exception as e:
        print(f"âŒ API Error: {e}")
        print(f"Retrying in 60s...")
        time.sleep(60)

# ==========================================
# ğŸ’¾ åˆå¹¶å¹¶ä¿å­˜
# ==========================================
print(f"\nğŸ’¾ Merging and saving results...")

final_df = pd.concat(all_generated_samples, axis=0, ignore_index=True)
final_df.insert(0, 'synindex', range(len(final_df)))

# ==========================================
# ğŸ”„ åå‘æ˜ å°„ï¼ˆæ¢å¤åŸå§‹å€¼ï¼‰
# ==========================================
if params['USE_RANDOM_WORD']:
    print(f"\nğŸ”„ Reversing Unique Variable Mapping...")

    for col in CATEGORICAL_FEATURES:
        if col in final_df.columns and col in reverse_mapping_dict:
            final_df[col] = final_df[col].map(reverse_mapping_dict[col])

# ==========================================
# ğŸ’¾ ä¿å­˜æ–‡ä»¶
# ==========================================
os.makedirs(params['SAVE_DIR'], exist_ok=True)

output_csv = os.path.join(params['SAVE_DIR'], f"{params['DATA_NAME']}_samples.csv")
output_txt = os.path.join(params['SAVE_DIR'], f"{params['DATA_NAME']}_samples.txt")

final_df.to_csv(output_csv, index=False)

# ä¿å­˜ Prompt æ¨¡æ¿
with open(output_txt, 'w', encoding='utf-8') as f:
    f.write(initial_prompt)

print(f"\nâœ… Done! Synthetic data saved to: {output_csv}")
print(f"ğŸ“Š Total samples generated: {len(final_df)}")
print(f"âŒ Parsing errors: {n_errors}")

print("\n" + "="*60)
print("ğŸ‰ ç”Ÿæˆå®Œæˆï¼")
print("="*60)

